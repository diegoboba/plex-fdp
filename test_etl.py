#!/usr/bin/env python3
"""
Test ETL pipeline con muestras pequeÃ±as
"""

import os
import sys

# Configurar PYTHONPATH
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from database.connector import DatabaseConnector
from etl.extractor import DataExtractor
from cloud.storage import StorageManager
from cloud.bigquery import BigQueryManager

def test_small_extraction():
    """Test extracciÃ³n con samples pequeÃ±os"""
    print("ğŸ” Testing small data extraction...")
    
    project_id = os.getenv('GCP_PROJECT_ID', 'plex-etl-project')
    db = DatabaseConnector(project_id)
    
    # Test con queries limitadas
    test_queries = {
        'plex_sucursales': "SELECT * FROM sucursales LIMIT 5",
        'plex_medicamentos': "SELECT * FROM medicamentos LIMIT 10", 
        'quantio_productos': "SELECT * FROM productos LIMIT 10"
    }
    
    extracted_data = {}
    
    for table_name, query in test_queries.items():
        try:
            database = table_name.split('_')[0]  # plex o quantio
            table = table_name.split('_', 1)[1]   # nombre de tabla
            
            print(f"  ğŸ“Š Extracting {table_name}...")
            df = db.extract_table_data(database, table, query)
            extracted_data[table_name] = df
            print(f"    âœ… {len(df)} rows extracted")
            
        except Exception as e:
            print(f"    âŒ Failed: {e}")
    
    return extracted_data

def test_storage_upload(data):
    """Test upload to Cloud Storage"""
    print("\nâ˜ï¸ Testing Cloud Storage upload...")
    
    try:
        storage = StorageManager()
        uploaded_files = storage.upload_all_data(data, file_format='csv')
        
        print(f"âœ… {len(uploaded_files)} files uploaded:")
        for table, path in uploaded_files.items():
            print(f"  - {table}: {path}")
            
        return uploaded_files
        
    except Exception as e:
        print(f"âŒ Storage upload failed: {e}")
        return {}

def test_bigquery_setup(uploaded_files):
    """Test BigQuery external tables"""
    print("\nğŸ—ï¸ Testing BigQuery setup...")
    
    try:
        bq = BigQueryManager()
        
        # Crear dataset si no existe
        bq.create_dataset_if_not_exists()
        
        # Crear external tables
        bq.create_all_external_tables(uploaded_files)
        
        print("âœ… BigQuery setup completed")
        return True
        
    except Exception as e:
        print(f"âŒ BigQuery setup failed: {e}")
        return False

def main():
    """Test completo del pipeline"""
    print("ğŸ§ª Testing ETL Pipeline with small samples...\n")
    
    # Step 1: Extract small samples
    data = test_small_extraction()
    if not data:
        print("âŒ No data extracted. Stopping.")
        return
    
    # Step 2: Upload to Storage
    uploaded_files = test_storage_upload(data)
    if not uploaded_files:
        print("âŒ Upload failed. Stopping.")
        return
    
    # Step 3: Setup BigQuery
    bq_success = test_bigquery_setup(uploaded_files)
    if not bq_success:
        print("âŒ BigQuery setup failed. Stopping.")
        return
    
    print("\nğŸ‰ ETL Pipeline test completed successfully!")
    print("\nğŸ“‹ Next steps:")
    print("1. Check BigQuery console for external tables")
    print("2. Run full ETL: cd src && python main.py")
    print("3. Deploy to Cloud Functions")

if __name__ == "__main__":
    main()