#!/usr/bin/env python3
"""
Script para re-procesar solo las tablas que fallaron (cargaron solo 300K filas)
"""

import os
import sys
import argparse
from datetime import datetime

# Add src to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.etl.streaming_extractor import StreamingDataExtractor
from src.cloud.bigquery import BigQueryManager

# Tablas que necesitan re-procesamiento (solo cargaron 300K filas)
TABLES_TO_REPROCESS = [
    'asientos_detalle',
    'factcabecera', 
    'factlineas',
    'reccabecera'
]

def reprocess_specific_tables(tables_list: list = None, chunk_size: int = 50000):
    """
    Re-procesa solo las tablas especificadas con full refresh
    
    Args:
        tables_list: Lista de tablas a re-procesar (sin prefijo plex_)
        chunk_size: Tama√±o del chunk para el procesamiento
    """
    tables = tables_list or TABLES_TO_REPROCESS
    
    print(f"üîÑ REPROCESAMIENTO DE TABLAS FALLIDAS")
    print(f"=" * 60)
    print(f"üìã Tablas a re-procesar: {', '.join(tables)}")
    print(f"üì¶ Chunk size: {chunk_size:,} filas")
    print(f"üïê Inicio: {datetime.now()}")
    print(f"=" * 60)
    
    # Initialize components
    extractor = StreamingDataExtractor()
    extractor.override_chunk_size = chunk_size
    bq_manager = BigQueryManager()
    
    results = {}
    total_rows = 0
    
    for table_name in tables:
        try:
            print(f"\n{'='*60}")
            print(f"üîÑ Re-procesando tabla: {table_name}")
            print(f"{'='*60}")
            
            # Determinar la base de datos (todas estas son de plex)
            database_name = 'plex'
            bq_table_name = f"plex_{table_name}"
            
            # Forzar TRUNCATE para limpiar datos parciales anteriores
            print(f"üóëÔ∏è  Truncando tabla existente {bq_table_name}...")
            
            # Extraer y cargar con el nuevo c√≥digo mejorado
            # Usar el m√©todo p√∫blico extract_and_load_table_streaming
            rows_loaded = extractor.extract_and_load_table_streaming(
                database_name=database_name,
                table_name=table_name,
                bq_table_name=bq_table_name,
                chunk_size=chunk_size,
                truncate_target=True,  # IMPORTANTE: Truncar datos anteriores
                query=None  # Full table extraction
            )
            
            results[table_name] = rows_loaded
            total_rows += rows_loaded
            
            print(f"‚úÖ {table_name}: {rows_loaded:,} filas cargadas")
            
        except Exception as e:
            print(f"‚ùå Error procesando {table_name}: {str(e)}")
            results[table_name] = 0
    
    # Resumen final
    print(f"\n{'='*60}")
    print(f"üìä RESUMEN DE REPROCESAMIENTO")
    print(f"{'='*60}")
    
    for table, rows in results.items():
        status = "‚úÖ" if rows > 300000 else "‚ö†Ô∏è"
        print(f"{status} {table}: {rows:,} filas")
    
    print(f"\nüìà Total: {total_rows:,} filas procesadas")
    print(f"üïê Fin: {datetime.now()}")
    
    return results

def main():
    parser = argparse.ArgumentParser(
        description='Re-procesar tablas que solo cargaron 300K filas'
    )
    parser.add_argument(
        '--tables', 
        nargs='+',
        help='Lista de tablas espec√≠ficas a re-procesar (sin prefijo plex_)',
        default=TABLES_TO_REPROCESS
    )
    parser.add_argument(
        '--chunk-size',
        type=int,
        default=50000,
        help='Tama√±o del chunk (default: 50000)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Solo mostrar qu√© se har√≠a sin ejecutar'
    )
    
    args = parser.parse_args()
    
    # Set up credentials
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'etl-service-account-key.json'
    
    if args.dry_run:
        print("üîç DRY RUN - Solo mostrando lo que se har√≠a:")
        print(f"Tablas a procesar: {args.tables}")
        print(f"Chunk size: {args.chunk_size:,}")
        return
    
    # Confirmar antes de proceder
    print(f"‚ö†Ô∏è  ATENCI√ìN: Se van a re-procesar {len(args.tables)} tablas:")
    for t in args.tables:
        print(f"  - {t}")
    print(f"\nChunk size: {args.chunk_size:,}")
    print("\nEsto TRUNCAR√Å los datos existentes y los reemplazar√°.")
    
    response = input("\n¬øContinuar? (s/n): ")
    if response.lower() != 's':
        print("Cancelado.")
        return
    
    # Ejecutar reprocesamiento
    results = reprocess_specific_tables(
        tables_list=args.tables,
        chunk_size=args.chunk_size
    )
    
    # Verificar √©xito
    failed = [t for t, r in results.items() if r <= 300000]
    if failed:
        print(f"\n‚ö†Ô∏è  Las siguientes tablas a√∫n tienen problemas: {failed}")
        print("Considera usar un chunk_size m√°s peque√±o o revisar la conexi√≥n.")
    else:
        print(f"\n‚úÖ Todas las tablas se reprocesaron exitosamente!")

if __name__ == "__main__":
    main()